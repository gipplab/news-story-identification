{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d77293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f562a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6db80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(df):\n",
    "    overall = True\n",
    "    for index, row in df.iterrows():\n",
    "        if len(str(row['body'])) == 0 or len(str(row['headline'])) == 0:\n",
    "            overall = False\n",
    "            print(f'==== Failed case: {row}')\n",
    "            \n",
    "    print(f'==== Overall result: {overall} ====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cb694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(df, size=10000):\n",
    "    return df.sample(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51bfcd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating file 2017_1.csv...\n",
      "==== Overall result: True ====\n",
      "Validating file 2017_2.csv...\n",
      "==== Overall result: True ====\n",
      "Validating file 2018_1.csv...\n",
      "==== Overall result: True ====\n",
      "Validating file 2018_2.csv...\n",
      "==== Overall result: True ====\n",
      "Validating file 2019_1.csv...\n",
      "==== Overall result: True ====\n",
      "Validating file 2019_2.csv...\n",
      "==== Overall result: True ====\n"
     ]
    }
   ],
   "source": [
    "files = ['2017_1.csv', '2017_2.csv', '2018_1.csv', '2018_2.csv', '2019_1.csv', '2019_2.csv']\n",
    "path = './data/polusa_balanced/'\n",
    "for file in files:\n",
    "    df = pd.read_csv(f'{path}{file}')\n",
    "#     print(df.keys())\n",
    "    print(f'Validating file {file}...')\n",
    "    validate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7390eaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start converting documents to *.txt\n",
      "10000 documents converted...\n",
      "20000 documents converted...\n",
      "30000 documents converted...\n",
      "40000 documents converted...\n",
      "50000 documents converted...\n",
      "60000 documents converted...\n",
      "70000 documents converted...\n",
      "80000 documents converted...\n",
      "90000 documents converted...\n",
      "100000 documents converted...\n",
      "110000 documents converted...\n",
      "120000 documents converted...\n",
      "130000 documents converted...\n",
      "140000 documents converted...\n",
      "150000 documents converted...\n",
      "160000 documents converted...\n",
      "170000 documents converted...\n",
      "180000 documents converted...\n",
      "190000 documents converted...\n",
      "200000 documents converted...\n",
      "210000 documents converted...\n",
      "220000 documents converted...\n",
      "230000 documents converted...\n",
      "240000 documents converted...\n",
      "250000 documents converted...\n",
      "260000 documents converted...\n",
      "270000 documents converted...\n",
      "280000 documents converted...\n",
      "290000 documents converted...\n",
      "300000 documents converted...\n",
      "310000 documents converted...\n",
      "320000 documents converted...\n",
      "330000 documents converted...\n",
      "340000 documents converted...\n",
      "350000 documents converted...\n",
      "360000 documents converted...\n",
      "370000 documents converted...\n",
      "380000 documents converted...\n",
      "390000 documents converted...\n",
      "400000 documents converted...\n",
      "410000 documents converted...\n",
      "420000 documents converted...\n",
      "430000 documents converted...\n",
      "440000 documents converted...\n",
      "450000 documents converted...\n",
      "460000 documents converted...\n",
      "470000 documents converted...\n",
      "480000 documents converted...\n",
      "490000 documents converted...\n",
      "500000 documents converted...\n",
      "510000 documents converted...\n",
      "520000 documents converted...\n",
      "530000 documents converted...\n",
      "540000 documents converted...\n",
      "550000 documents converted...\n",
      "560000 documents converted...\n",
      "570000 documents converted...\n",
      "580000 documents converted...\n",
      "590000 documents converted...\n",
      "600000 documents converted...\n",
      "610000 documents converted...\n",
      "620000 documents converted...\n",
      "630000 documents converted...\n",
      "640000 documents converted...\n",
      "650000 documents converted...\n",
      "660000 documents converted...\n",
      "670000 documents converted...\n",
      "680000 documents converted...\n",
      "690000 documents converted...\n",
      "700000 documents converted...\n",
      "710000 documents converted...\n",
      "720000 documents converted...\n",
      "730000 documents converted...\n",
      "740000 documents converted...\n",
      "750000 documents converted...\n",
      "760000 documents converted...\n",
      "770000 documents converted...\n",
      "780000 documents converted...\n",
      "790000 documents converted...\n",
      "800000 documents converted...\n",
      "810000 documents converted...\n",
      "820000 documents converted...\n",
      "830000 documents converted...\n",
      "840000 documents converted...\n",
      "850000 documents converted...\n",
      "860000 documents converted...\n",
      "870000 documents converted...\n",
      "880000 documents converted...\n",
      "890000 documents converted...\n",
      "900000 documents converted...\n",
      "=== FINISHED === Total of 905573 documents converted. 0:34:21.262215 elapsed\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "doc_cnt = 1\n",
    "out_dir = './data/polusa_raw'\n",
    "print('Start converting documents to *.txt')\n",
    "start = datetime.now()\n",
    "for file in files:\n",
    "    df = pd.read_csv(f'{path}{file}')\n",
    "    for index, row in df.iterrows():\n",
    "        content = row['body']\n",
    "        with open(f'{out_dir}/document_{str(doc_cnt).zfill(6)}.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "            f.close()\n",
    "        doc_cnt = doc_cnt + 1\n",
    "        if doc_cnt % 10000 == 0:\n",
    "            print(f'{doc_cnt} documents converted...')\n",
    "print(f'=== FINISHED === Total of {doc_cnt} documents converted. {datetime.now() - start } elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71ea802d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading 2017_1.csv ===\n",
      "=== Reading 2017_2.csv ===\n",
      "=== Reading 2018_1.csv ===\n",
      "=== Reading 2018_2.csv ===\n",
      "=== Reading 2019_1.csv ===\n",
      "=== Reading 2019_2.csv ===\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for file in files:\n",
    "    print(f'=== Reading {file} ===')\n",
    "    df = pd.read_csv(f'{path}{file}')\n",
    "    dfs.append(minimize(df, size=20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83882212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Merging all dataframes ===\n"
     ]
    }
   ],
   "source": [
    "print(f'=== Merging all dataframes ===')\n",
    "merged = pd.concat(dfs, keys=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fbe0eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Writing to 2017_1.csv ===\n",
      "=== Writing to 2017_2.csv ===\n",
      "=== Writing to 2018_1.csv ===\n",
      "=== Writing to 2018_2.csv ===\n",
      "=== Writing to 2019_1.csv ===\n",
      "=== Writing to 2019_2.csv ===\n",
      "=== Finished ===\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(files)):\n",
    "    print(f'=== Writing to {files[i]} ===')\n",
    "    dfs[i].to_csv(files[i], encoding=\"utf-8\")\n",
    "# merged.to_csv(file_name, encoding=\"utf-8\")\n",
    "print(f'=== Finished ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621f56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
