{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008f3b0e",
   "metadata": {},
   "source": [
    "## Link\n",
    "\n",
    "#### https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cdba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def import_data(data_dir, limit=-1):\n",
    "    doc_cnt = 1\n",
    "    data = []\n",
    "    for item in listdir(data_dir):\n",
    "        if isfile(join(data_dir, item)):\n",
    "            doc_cnt = doc_cnt + 1\n",
    "            if doc_cnt == limit:\n",
    "                break\n",
    "            with open(f'{data_dir}{item}', 'r', encoding='utf-8') as f:\n",
    "                data.append(f.read())\n",
    "                if doc_cnt % 10000 == 0:\n",
    "                    print(f'{doc_cnt} document(s) read...')\n",
    "                f.close()\n",
    "    return data\n",
    "data_dir = '../data/polusa_raw/'\n",
    "data = import_data(data_dir, limit=1000)\n",
    "\n",
    "    \n",
    "print(f'=== total {len(data)} documents read')\n",
    "\n",
    "def clean_data(data):\n",
    "    print(f'Cleaning data... ', end='')\n",
    "    # Convert to list\n",
    "    # data = df.content.values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    print(f'removing emails... ', end='')\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    print(f'removing new line characters... ', end='')\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    print(f'removing single quotes... ', end='')\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "    print('finished !')\n",
    "    return data\n",
    "\n",
    "data = clean_data(data)\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "\n",
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 15, 20], \n",
    "                 'learning_decay': [.5, .7, .9],\n",
    "                 'learning_method': ['online']}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\n",
    "\n",
    "# GridSearchCV(cv=None, error_score='raise',\n",
    "#        estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "#              evaluate_every=-1, learning_decay=0.7, learning_method='online',\n",
    "#              learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "#              mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
    "#              #n_topics=None, #deprecated\n",
    "#              perp_tol=0.1, random_state=None,\n",
    "#              topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "#        #fit_params=None, #deprecated\n",
    "#        #iid=True,   #deprecated\n",
    "#        n_jobs=1,\n",
    "#        param_grid={\n",
    "#            'n_components': [5, 10, 15, 20], \n",
    "#            'learning_decay': [0.5, 0.7, 0.9]\n",
    "#        },\n",
    "#        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "#        scoring=None, verbose=0)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "\n",
    "n_components = [5, 10, 15, 20]\n",
    "# log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.cv_results_ if gscore.parameters['learning_decay']==0.5]\n",
    "for key in model.cv_results_:\n",
    "    print(key, end=' ')\n",
    "print(model.cv_results_)\n",
    "\n",
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [5, 10, 15, 20]\n",
    "log_likelyhoods_5 = [np.round(model.cv_results_['mean_test_score']) for key_score in model.cv_results_ if key_score == 'param_learning_decay' and model.cv_results_[key_score][0] == 0.5]\n",
    "log_likelyhoods_7 = [np.round(model.cv_results_['mean_test_score']) for key_score in model.cv_results_ if key_score == 'param_learning_decay' and model.cv_results_[key_score][0] == 0.7]\n",
    "log_likelyhoods_9 = [np.round(model.cv_results_['mean_test_score']) for key_score in model.cv_results_ if key_score == 'param_learning_decay' and model.cv_results_[key_score][0] == 0.9]\n",
    "\n",
    "log_likelyhoods_5 = []\n",
    "log_likelyhoods_7 = []\n",
    "log_likelyhoods_9 = []\n",
    "for i, learning_decay in enumerate(model.cv_results_['param_learning_decay']):\n",
    "    if learning_decay == 0.5:\n",
    "        log_likelyhoods_5.append(model.cv_results_['mean_test_score'][i])\n",
    "    elif learning_decay == 0.7:\n",
    "        log_likelyhoods_7.append(model.cv_results_['mean_test_score'][i])\n",
    "    elif learning_decay == 0.9:\n",
    "        log_likelyhoods_9.append(model.cv_results_['mean_test_score'][i])\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics\n",
    "\n",
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution\n",
    "\n",
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords.head()\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=None, n_words=20):\n",
    "    topic_keywords = []\n",
    "    if lda_model != None:\n",
    "        keywords = np.array(vectorizer.get_feature_names())\n",
    "        for topic_weights in lda_model.components_:\n",
    "            top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "            topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    else:\n",
    "        print(f'=== WARNING === lda_model is \"None\"')\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords\n",
    "\n",
    "# Construct the k-means clusters\n",
    "from sklearn.cluster import KMeans\n",
    "clusters = KMeans(n_clusters=15, random_state=100).fit_predict(lda_output)\n",
    "\n",
    "# Build the Singular Value Decomposition(SVD) model\n",
    "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "\n",
    "# X and Y axes of the plot using SVD decomposition\n",
    "x = lda_output_svd[:, 0]\n",
    "y = lda_output_svd[:, 1]\n",
    "\n",
    "# Weights for the 15 columns of lda_output, for each component\n",
    "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
    "\n",
    "# Percentage of total information in 'lda_output' explained by the two components\n",
    "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
